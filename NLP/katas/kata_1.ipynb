{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Cleaning Pipeline**\n",
    "\n",
    "**Scenario:** You're building a basic sentiment analysis tool for social media comments. The comments often contain typos, emojis, and irrelevant words. You need to clean the text before feeding it to your sentiment analysis model.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Text normalization:** Write a function that takes a comment string and performs the following:\n",
    "    - Lowercase all characters.\n",
    "    - Remove punctuation (except for exclamation points and question marks that might be sentiment indicators).\n",
    "    - Replace emojis with descriptive text (e.g., \"happy\" for ).\n",
    "2. **Tokenization:** Write a function that splits the normalized text into individual words (tokens).\n",
    "3. **Stop word removal:** Create a list of common stop words (e.g., \"the\", \"a\", \"is\"). Write a function that removes these stop words from the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/haria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_path, text_column='Text'):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if text_column not in df.columns:\n",
    "            raise KeyError(f\"Column '{text_column}' not found in CSV\")\n",
    "        return df[text_column].fillna('')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return pd.Series([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(comment_series):\n",
    "    \"\"\"Lowercases, removes punctuation (except '!' and '?'), and replaces emojis with descriptive text.\"\"\"\n",
    "    punctuation_remove = string.punctuation.replace('!', '').replace('?', '')\n",
    "    \n",
    "    comment_series = comment_series.str.lower() \n",
    "    comment_series = comment_series.str.translate(str.maketrans('', '', punctuation_remove)) \n",
    "    comment_series = comment_series.apply(emoji.demojize)\n",
    "    \n",
    "    return comment_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(comment_series):\n",
    "    \"\"\"Splits each comment into individual tokens (words).\"\"\"\n",
    "    return comment_series.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(token_series, custom_stop_words=None):\n",
    "    \"\"\"Removes stop words from tokenized comments. Accepts an optional custom stop-word list.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if custom_stop_words:\n",
    "        stop_words.update(custom_stop_words)\n",
    "    \n",
    "    return token_series.apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_pipeline(custom_stop_words=None):\n",
    "    \"\"\"Creates a text processing pipeline for normalization, tokenization, and stop-word removal.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('normalize', FunctionTransformer(lambda x: normalize_text(x), validate=False)),\n",
    "        ('tokenize', FunctionTransformer(lambda x: tokenize_text(x), validate=False)),\n",
    "        ('remove_stopwords', FunctionTransformer(lambda x: remove_stop_words(x, custom_stop_words), validate=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments_from_csv(file_path, text_column='Text', custom_stop_words=None):\n",
    "    \"\"\"Processes comments from CSV using the text pipeline and returns cleaned tokens.\"\"\"\n",
    "    comments = load_data_from_csv(file_path, text_column)\n",
    "    if comments.empty:\n",
    "        print(\"No comments to process.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    text_pipeline = create_text_pipeline(custom_stop_words)\n",
    "    cleaned_tokens = text_pipeline.fit_transform(comments)\n",
    "    \n",
    "    cleaned_df = pd.DataFrame({'Cleaned Tokens': cleaned_tokens})\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Cleaned Tokens\n",
      "0                    [enjoying, beautiful, day, park!]\n",
      "1                         [traffic, terrible, morning]\n",
      "2       [finished, amazing, workout!, :flexed_biceps:]\n",
      "3               [excited, upcoming, weekend, getaway!]\n",
      "4               [trying, new, recipe, dinner, tonight]\n",
      "..                                                 ...\n",
      "727  [collaborating, science, project, received, re...\n",
      "728  [attending, surprise, birthday, party, organiz...\n",
      "729  [successfully, fundraising, school, charity, i...\n",
      "730  [participating, multicultural, festival, celeb...\n",
      "731  [organizing, virtual, talent, show, challengin...\n",
      "\n",
      "[732 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'datasets/sentiment.csv'\n",
    "cleaned_tokens = process_comments_from_csv(csv_file_path)\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
