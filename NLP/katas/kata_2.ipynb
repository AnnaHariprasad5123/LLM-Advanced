{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "**Document Summarization**\n",
    "\n",
    "**Scenario**: You're working on a news aggregator that displays summaries of various articles. You want to identify the most important keywords in each article to generate concise summaries.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **TF-IDF calculation**: Implement the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm. This involves calculating the frequency of each word in a document (TF) and how rare it is across all documents (IDF).\n",
    "2. **Document Summarization**: Write a function that takes a list of pre-processed news articles (cleaned text) as input. It should perform the following:\n",
    "    - Calculate TF-IDF for each word in each document.\n",
    "    - Identify the top N words (keywords) with the highest TF-IDF scores for each document.\n",
    "    - Generate a summary sentence using these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haria/Documents/Learnings/LLM Advanced/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apr 23 2024</td>\n",
       "      <td>UFOs and SGU on John Oliver</td>\n",
       "      <td>Steven Novella</td>\n",
       "      <td>UFO's / Aliens</td>\n",
       "      <td>The most recent episode of John Oliver, Last W...</td>\n",
       "      <td>https://theness.com/neurologicablog/ufos-and-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apr 22 2024</td>\n",
       "      <td>Indigenous Knowledge</td>\n",
       "      <td>Steven Novella</td>\n",
       "      <td>Culture and Society</td>\n",
       "      <td>I recently received the following question to ...</td>\n",
       "      <td>https://theness.com/neurologicablog/indigenous...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication_date                        title          author  \\\n",
       "0      Apr 23 2024  UFOs and SGU on John Oliver  Steven Novella   \n",
       "1      Apr 22 2024         Indigenous Knowledge  Steven Novella   \n",
       "\n",
       "            categories                                               text  \\\n",
       "0       UFO's / Aliens  The most recent episode of John Oliver, Last W...   \n",
       "1  Culture and Society  I recently received the following question to ...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://theness.com/neurologicablog/ufos-and-s...  \n",
       "1  https://theness.com/neurologicablog/indigenous...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'datasets/neurologica_articles.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[:2]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/haria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(comment_series):\n",
    "    \"\"\"Lowercases, removes punctuation (except '!' and '?'), and replaces emojis with descriptive text.\"\"\"\n",
    "    punctuation_remove = string.punctuation.replace('!', '').replace('?', '')\n",
    "    \n",
    "    # Vectorized operations with pandas: lowercasing, removing punctuation, and replacing emojis\n",
    "    comment_series = comment_series.str.lower() \n",
    "    comment_series = comment_series.str.translate(str.maketrans('', '', punctuation_remove)) \n",
    "    comment_series = comment_series.apply(emoji.demojize)\n",
    "    \n",
    "    return comment_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(comment_series):\n",
    "    \"\"\"Splits each comment into individual tokens (words).\"\"\"\n",
    "    return comment_series.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(token_series, custom_stop_words=None):\n",
    "    \"\"\"Removes stop words from tokenized comments. Accepts an optional custom stop-word list.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if custom_stop_words:\n",
    "        stop_words.update(custom_stop_words)\n",
    "    \n",
    "    return token_series.apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_pipeline(custom_stop_words=None):\n",
    "    \"\"\"Creates a text processing pipeline for normalization, tokenization, and stop-word removal.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('normalize', FunctionTransformer(lambda x: normalize_text(x), validate=False)),\n",
    "        ('tokenize', FunctionTransformer(lambda x: tokenize_text(x), validate=False)),\n",
    "        ('remove_stopwords', FunctionTransformer(lambda x: remove_stop_words(x, custom_stop_words), validate=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apr 23 2024</td>\n",
       "      <td>UFOs and SGU on John Oliver</td>\n",
       "      <td>Steven Novella</td>\n",
       "      <td>UFO's / Aliens</td>\n",
       "      <td>The most recent episode of John Oliver, Last W...</td>\n",
       "      <td>https://theness.com/neurologicablog/ufos-and-s...</td>\n",
       "      <td>[recent, episode, john, oliver, last, week, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apr 22 2024</td>\n",
       "      <td>Indigenous Knowledge</td>\n",
       "      <td>Steven Novella</td>\n",
       "      <td>Culture and Society</td>\n",
       "      <td>I recently received the following question to ...</td>\n",
       "      <td>https://theness.com/neurologicablog/indigenous...</td>\n",
       "      <td>[recently, received, following, question, sgu,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication_date                        title          author  \\\n",
       "0      Apr 23 2024  UFOs and SGU on John Oliver  Steven Novella   \n",
       "1      Apr 22 2024         Indigenous Knowledge  Steven Novella   \n",
       "\n",
       "            categories                                               text  \\\n",
       "0       UFO's / Aliens  The most recent episode of John Oliver, Last W...   \n",
       "1  Culture and Society  I recently received the following question to ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://theness.com/neurologicablog/ufos-and-s...   \n",
       "1  https://theness.com/neurologicablog/indigenous...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  [recent, episode, john, oliver, last, week, to...  \n",
       "1  [recently, received, following, question, sgu,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_comments_from_csv(custom_stop_words=None):\n",
    "    text_pipeline = create_text_pipeline(custom_stop_words)\n",
    "    df[\"cleaned_text\"] = text_pipeline.fit_transform(df[\"text\"])\n",
    "    \n",
    "process_comments_from_csv()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bart_model():\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    # model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a summary using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, tokenizer, model, max_length=130, min_length=30):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=max_length, min_length=min_length, length_penalty=2.0)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate TF-IDF scores for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract top N keywords based on TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(tfidf_matrix, feature_names, top_n=5):\n",
    "    top_keywords_per_doc = []\n",
    "    \n",
    "    for doc_index in range(tfidf_matrix.shape[0]):\n",
    "        tfidf_row = tfidf_matrix[doc_index].toarray().flatten()\n",
    "        top_indices = np.argsort(tfidf_row)[-top_n:]\n",
    "        top_keywords = [feature_names[index] for index in top_indices]\n",
    "        top_keywords_per_doc.append(top_keywords)\n",
    "    \n",
    "    return top_keywords_per_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(df, top_n_keywords=5):\n",
    "    \n",
    "    documents = df['text'].tolist()\n",
    "    \n",
    "    cleaned_documents = [' '.join(tokens) for tokens in df['cleaned_text']]\n",
    "    \n",
    "    tokenizer, model = load_bart_model()\n",
    "    \n",
    "    tfidf_matrix, feature_names = calculate_tfidf(cleaned_documents)\n",
    "    \n",
    "    top_keywords_per_doc = get_top_keywords(tfidf_matrix, feature_names, top_n=top_n_keywords)\n",
    "    \n",
    "    summaries = []\n",
    "    for i, keywords in enumerate(top_keywords_per_doc):\n",
    "        \n",
    "        keyword_sentence = \" \".join(keywords)\n",
    "        structured_input = f\"The article discusses: {keyword_sentence}.\"\n",
    "        \n",
    "        key_words_summary = summarize(structured_input, tokenizer, model)\n",
    "        \n",
    "        normal_summary = summarize(documents[i], tokenizer, model)\n",
    "        \n",
    "        summaries.append({\n",
    "            'title': df['title'][i],\n",
    "            'key_words_summary': key_words_summary,\n",
    "            'normal_summary': normal_summary\n",
    "        })\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 15:25:05.319161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734515705.489949  193919 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734515705.530615  193919 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 15:25:05.885926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 56 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: UFOs and SGU on John Oliver\n",
      "Generated Summary with only key words: the case of a young woman who was believed to have been shot and killed by an erythrocyte membranejoy group known as the erythrocyte extrapyramidal group is reported .<n> the patient , who was a follower of the group , has not been found .<n> the case has attracted considerable controversy , with some claiming there was an error in initial investigation and others questioning the authenticity of the findings . in this article , the author discusses the case with particular reference to the media , researchers and his former colleagues on both the pro and con sides .\n",
      "\n",
      "Generated Summary with whole text: abstractthe mainstream media has a habit of giving space - based phenomena or , more generally , what has been termed space or , more specifically , what has been described as , what has been described as , what has been described as , what has been described as , what has been described as , what has been described as being or what has been described as being. this habit of giving pre - canned , what has been described as , what has been described as , what has been described as being or what has been described as being before gives atypically powerful arguments for or against what has been\n",
      "\n",
      "Title: Indigenous Knowledge\n",
      "Generated Summary with only key words: the transcultural human ideal , or common humanity , is a fundamental concept that seeks to bridge the gap between what is common and what is not common .<n> common humanity , or common humanity , is a set of related human beings ( e.g. , chimpanzees , gorillas , humans , chimpanzees , and humans ) whose common denominator is that they all have the same physical , cultural , religious , social , and biological characteristics .<n> the ideal , or common humanity , has been present in nature for at least as long as the earliest common ancestor (\n",
      "\n",
      "Generated Summary with whole text: when thinking about colonization we often stop to think about the effects it has on other people .<n> when thinking about indigenous cultures , we stop to think about the effects it has on other people .<n> but when it comes to thinking about human beings , there are some issues that are clearly different between what we think about indigenous cultures and what we think about human beings .<n> one issue that is often overlooked or simply not considered when thinking about indigenous cultures is the impact of colonization on other peoples .<n> when we think about indigenous cultures , we usually think about the effects it has on other peoples .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = process_articles(df, top_n_keywords=45)\n",
    "\n",
    "# Output summaries\n",
    "for summary in summaries:\n",
    "    print(f\"Title: {summary['title']}\")\n",
    "    print(f\"Generated Summary with only key words: {summary['key_words_summary']}\\n\")\n",
    "    print(f\"Generated Summary with whole text: {summary['normal_summary']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
